---
title: "LogicCat: Text-to-SQL Benchmark for Multi-Domain Reasoning Challenges"
collection: publications
category: manuscripts
permalink: /publication/2025-07-01-logiccat
excerpt: 'A comprehensive benchmark for evaluating Text-to-SQL systems on multi-domain reasoning challenges.'
date: 2025-07-01
venue: 'ArXiv Preprint'
paperurl: 'https://arxiv.org/abs/your-paper-id'
citation: 'Mao, X. H., et al. (2025). &quot;LogicCat: Text-to-SQL Benchmark for Multi-Domain Reasoning Challenges.&quot; <i>ArXiv Preprint</i>.'
---
This paper introduces LogicCat, a comprehensive benchmark for evaluating Text-to-SQL systems on multi-domain reasoning challenges. The benchmark provides a standardized evaluation framework for complex SQL generation tasks, enabling systematic assessment of model performance across diverse reasoning scenarios. 